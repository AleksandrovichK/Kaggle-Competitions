{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">  DISCLAIMER </h1>\n<p>Guys, everything that you see in the kernel you can freely take, copy, modify and create your own amazing solutions!</p>\n<p>If you don't want to waste your time - just read 'briefly' section I made for you.</p>\n<p>Do not forget upvote if the kernel was useful for you.</p>\n<img src=\"https://i.gifer.com/Be.gif\">"},{"metadata":{},"cell_type":"markdown","source":"<h1>TODO-list</h1>\n<ul>\n    <li>Add more new features</li>\n    <li>Remove outliers</li>\n    <li>Try to stack some models or just investigate how to do it</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Imports\n</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport warnings\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom mlxtend.regressor import StackingCVRegressor\nfrom scipy.stats import skew\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n\nprint(\"Imports have been set\")\n\n# Disabling warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":63,"outputs":[{"output_type":"stream","text":"Imports have been set\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Input data handling\n</h1>\n<span> Briefly:\n    <ul>\n        <li> removing rows with NaN in Sale Price </li>\n        <li> logarithm SalePrice (and when model predicts I use np.expm1 function to return value)  </li>\n        <li> splitting X to target variable y and train_features </li>\n        <li> joining X_test and train_features to process all features together </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the training/val data and the test data\nX = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')\n\n# Rows before:\nrows_before = X.shape[0]\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\nrows_after = X.shape[0]\nprint(\"\\nRows containing NaN in SalePrice were dropped: \" + str(rows_before - rows_after))\n\n# Logarithming target variable in order to make distribution better\nX['SalePrice'] = np.log1p(X['SalePrice'])\n\ny = X['SalePrice'].reset_index(drop=True)\ntrain_features = X.drop(['SalePrice'], axis=1)\n\n# concatenate the train and the test set as features for tranformation to avoid mismatch\nfeatures = pd.concat([train_features, X_test]).reset_index(drop=True)\nprint('\\nFeatures size:', features.shape)","execution_count":64,"outputs":[{"output_type":"stream","text":"\nRows containing NaN in SalePrice were dropped: 0\n\nFeatures size: (2919, 79)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Checking for NaNs and printing them\n</h1>\n<span> Briefly:\n    <ul>\n        <li> printing NaN-containing columns names </li>\n        <li> printing NaN-containing columns values for clarity</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_count_table = (features.isnull().sum())\nnan_count_table = nan_count_table[nan_count_table > 0].sort_values(ascending=False)\nprint(\"\\nColums containig NaN: \")\nprint(nan_count_table)\n\ncolumns_containig_nan = nan_count_table.index.to_list()\nprint(\"\\nWhat values they contain: \")\nprint(features[columns_containig_nan])","execution_count":65,"outputs":[{"output_type":"stream","text":"\nColums containig NaN: \nPoolQC          2909\nMiscFeature     2814\nAlley           2721\nFence           2348\nFireplaceQu     1420\nLotFrontage      486\nGarageFinish     159\nGarageYrBlt      159\nGarageQual       159\nGarageCond       159\nGarageType       157\nBsmtExposure      82\nBsmtCond          82\nBsmtQual          81\nBsmtFinType2      80\nBsmtFinType1      79\nMasVnrType        24\nMasVnrArea        23\nMSZoning           4\nBsmtFullBath       2\nBsmtHalfBath       2\nUtilities          2\nFunctional         2\nExterior2nd        1\nExterior1st        1\nSaleType           1\nBsmtFinSF1         1\nBsmtFinSF2         1\nBsmtUnfSF          1\nElectrical         1\nKitchenQual        1\nGarageCars         1\nGarageArea         1\nTotalBsmtSF        1\ndtype: int64\n\nWhat values they contain: \n     PoolQC MiscFeature Alley  Fence FireplaceQu  LotFrontage GarageFinish  \\\n0       NaN         NaN   NaN    NaN         NaN         65.0          RFn   \n1       NaN         NaN   NaN    NaN          TA         80.0          RFn   \n2       NaN         NaN   NaN    NaN          TA         68.0          RFn   \n3       NaN         NaN   NaN    NaN          Gd         60.0          Unf   \n4       NaN         NaN   NaN    NaN          TA         84.0          RFn   \n...     ...         ...   ...    ...         ...          ...          ...   \n2914    NaN         NaN   NaN    NaN         NaN         21.0          NaN   \n2915    NaN         NaN   NaN    NaN         NaN         21.0          Unf   \n2916    NaN         NaN   NaN    NaN          TA        160.0          Unf   \n2917    NaN        Shed   NaN  MnPrv         NaN         62.0          NaN   \n2918    NaN         NaN   NaN    NaN          TA         74.0          Fin   \n\n      GarageYrBlt GarageQual GarageCond  ... Exterior1st SaleType BsmtFinSF1  \\\n0          2003.0         TA         TA  ...     VinylSd       WD      706.0   \n1          1976.0         TA         TA  ...     MetalSd       WD      978.0   \n2          2001.0         TA         TA  ...     VinylSd       WD      486.0   \n3          1998.0         TA         TA  ...     Wd Sdng       WD      216.0   \n4          2000.0         TA         TA  ...     VinylSd       WD      655.0   \n...           ...        ...        ...  ...         ...      ...        ...   \n2914          NaN        NaN        NaN  ...     CemntBd       WD        0.0   \n2915       1970.0         TA         TA  ...     CemntBd       WD      252.0   \n2916       1960.0         TA         TA  ...     VinylSd       WD     1224.0   \n2917          NaN        NaN        NaN  ...     HdBoard       WD      337.0   \n2918       1993.0         TA         TA  ...     HdBoard       WD      758.0   \n\n     BsmtFinSF2 BsmtUnfSF Electrical KitchenQual  GarageCars GarageArea  \\\n0           0.0     150.0      SBrkr          Gd         2.0      548.0   \n1           0.0     284.0      SBrkr          TA         2.0      460.0   \n2           0.0     434.0      SBrkr          Gd         2.0      608.0   \n3           0.0     540.0      SBrkr          Gd         3.0      642.0   \n4           0.0     490.0      SBrkr          Gd         3.0      836.0   \n...         ...       ...        ...         ...         ...        ...   \n2914        0.0     546.0      SBrkr          TA         0.0        0.0   \n2915        0.0     294.0      SBrkr          TA         1.0      286.0   \n2916        0.0       0.0      SBrkr          TA         2.0      576.0   \n2917        0.0     575.0      SBrkr          TA         0.0        0.0   \n2918        0.0     238.0      SBrkr          TA         3.0      650.0   \n\n      TotalBsmtSF  \n0           856.0  \n1          1262.0  \n2           920.0  \n3           756.0  \n4          1145.0  \n...           ...  \n2914        546.0  \n2915        546.0  \n2916       1224.0  \n2917        912.0  \n2918        996.0  \n\n[2919 rows x 34 columns]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Feature engineering\n</h1>\n<span> Briefly:\n    <ul>\n        <li> Filling with 0 numeric columns </li>\n        <li> Filling with 'None' categoric columns where 'NA' meant 'other' value</li>\n        <li> Filling with the most frequent values categoric columns where 'NA' meant 'nothing is here'</li>\n        <li> Turning to 'str' columns which are actually categoric </li>\n        <li> Turning to 'int' columns which are actually numeric </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in columns_containig_nan:\n\n    # populating with 0\n    if column in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                  'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'TotalBsmtSF',\n                  'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold',\n                  'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea']:\n        features[column] = features[column].fillna(0)\n\n    # populate with 'None'\n    if column in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \"PoolQC\", 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                  'BsmtFinType2', 'Neighborhood', 'BldgType', 'HouseStyle', 'MasVnrType', 'FireplaceQu', 'Fence', 'MiscFeature']:\n        features[column] = features[column].fillna('None')\n\n    # populate with most frequent value for cateforic\n    if column in ['Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'RoofStyle',\n                  'Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'RoofMatl', 'ExterQual', 'ExterCond',\n                  'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']:\n        features[column] = features[column].fillna(features[column].mode()[0])\n\n# MSSubClass: Numeric feature. Identifies the type of dwelling involved in the sale.\n#     20  1-STORY 1946 & NEWER ALL STYLES\n#     30  1-STORY 1945 & OLDER\n#     40  1-STORY W/FINISHED ATTIC ALL AGES\n#     45  1-1/2 STORY - UNFINISHED ALL AGES\n#     50  1-1/2 STORY FINISHED ALL AGES\n#     60  2-STORY 1946 & NEWER\n#     70  2-STORY 1945 & OLDER\n#     75  2-1/2 STORY ALL AGES\n#     80  SPLIT OR MULTI-LEVEL\n#     85  SPLIT FOYER\n#     90  DUPLEX - ALL STYLES AND AGES\n#    120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n#    150  1-1/2 STORY PUD - ALL AGES\n#    160  2-STORY PUD - 1946 & NEWER\n#    180  PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n#    190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\n# Stored as number so converted to string.\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures[\"MSSubClass\"] = features[\"MSSubClass\"].fillna(\"Unknown\")\n# MSZoning: Identifies the general zoning classification of the sale.\n#    A    Agriculture\n#    C    Commercial\n#    FV   Floating Village Residential\n#    I    Industrial\n#    RH   Residential High Density\n#    RL   Residential Low Density\n#    RP   Residential Low Density Park\n#    RM   Residential Medium Density\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# LotFrontage: Linear feet of street connected to property\n# Groupped by neighborhood and filled in missing value by the median LotFrontage of all the neighborhood\n# TODO may be 0 would perform better than median?\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n# LotArea: Lot size in square feet.\n# Stored as string so converted to int.\nfeatures['LotArea'] = features['LotArea'].astype(np.int64)\n# Alley: Type of alley access to property\n#    Grvl Gravel\n#    Pave Paved\n#    NA   No alley access\n\n# So. If 'Street' made of 'Pave', so it would be reasonable to assume that 'Alley' might be 'Pave' as well.\nfeatures['Alley'] = features['Alley'].fillna('Pave')\n# MasVnrArea: Masonry veneer area in square feet\n# Stored as string so converted to int.\nfeatures['MasVnrArea'] = features['MasVnrArea'].astype(np.int64)","execution_count":66,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Adding new features\n</h1>\n<span> Briefly:\n    <ul>\n        <li> YrBltAndRemod means overall sum of years </li>\n        <li> Separating to the other features overall squares</li>\n        <li> Separating to the other features presence/absence of a garage and so on</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# If area is not 0 so creating new feature looks reasonable\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint('Features size:', features.shape)","execution_count":67,"outputs":[{"output_type":"stream","text":"Features size: (2919, 89)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Let's check if we filled all the gaps\n</h1>\n<span> Briefly:\n    <ul>\n        <li> Just printing True or False if all the gaps are filled </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_count_train_table = (features.isnull().sum())\nnan_count_train_table = nan_count_train_table[nan_count_train_table > 0].sort_values(ascending=False)\nprint(\"\\nAre no NaN here now: \" + str(nan_count_train_table.size == 0))","execution_count":68,"outputs":[{"output_type":"stream","text":"\nAre no NaN here now: True\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Fixing skewed values\n</h1>\n<span> Briefly:\n    <ul>\n        <li> Checking skewness of all the numeric features and logarithm it if more than 0.5 </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = [cname for cname in features.columns if features[cname].dtype in ['int64', 'float64']]\nprint(\"\\nColumns which are numeric: \" + str(len(numeric_columns)) + \" out of \" + str(features.shape[1]))\nprint(numeric_columns)\n\ncategoric_columns = [cname for cname in features.columns if features[cname].dtype == \"object\"]\nprint(\"\\nColumns whice are categoric: \" + str(len(categoric_columns)) + \" out of \" + str(features.shape[1]))\nprint(categoric_columns)\n\nskewness = features[numeric_columns].apply(lambda x: skew(x))\nprint(skewness.sort_values(ascending=False))\n\nskewness = skewness[abs(skewness) > 0.5]\nfeatures[skewness.index] = np.log1p(features[skewness.index])\nprint(\"\\nSkewed values: \" + str(skewness.index))","execution_count":69,"outputs":[{"output_type":"stream","text":"\nColumns which are numeric: 45 out of 89\n['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'YrBltAndRemod', 'TotalSF', 'Total_sqr_footage', 'Total_Bathrooms', 'Total_porch_sf', 'haspool', 'has2ndfloor', 'hasgarage', 'hasbsmt', 'hasfireplace']\n\nColumns whice are categoric: 44 out of 89\n['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\nMiscVal              21.947195\nPoolArea             16.898328\nhaspool              14.884318\nLotArea              12.822431\nLowQualFinSF         12.088761\n3SsnPorch            11.376065\nKitchenAbvGr          4.302254\nBsmtFinSF2            4.146143\nEnclosedPorch         4.003891\nScreenPorch           3.946694\nBsmtHalfBath          3.931594\nMasVnrArea            2.613592\nOpenPorchSF           2.535114\nWoodDeckSF            1.842433\nTotal_sqr_footage     1.834437\nTotalSF               1.511479\nLotFrontage           1.505704\n1stFlrSF              1.469604\nBsmtFinSF1            1.425230\nTotal_porch_sf        1.376649\nGrLivArea             1.269358\nTotalBsmtSF           1.156894\nBsmtUnfSF             0.919339\n2ndFlrSF              0.861675\nTotRmsAbvGrd          0.758367\nFireplaces            0.733495\nHalfBath              0.694566\nBsmtFullBath          0.624832\nOverallCond           0.570312\nTotal_Bathrooms       0.492247\nBedroomAbvGr          0.326324\nhas2ndfloor           0.288675\nGarageArea            0.239257\nOverallQual           0.197110\nMoSold                0.195884\nFullBath              0.167606\nYrSold                0.132399\nhasfireplace         -0.054148\nGarageCars           -0.219581\nYrBltAndRemod        -0.304307\nYearRemodAdd         -0.451020\nYearBuilt            -0.599806\nGarageYrBlt          -3.906205\nhasgarage            -3.941054\nhasbsmt              -5.828995\ndtype: float64\n\nSkewed values: Index(['LotFrontage', 'LotArea', 'OverallCond', 'YearBuilt', 'MasVnrArea',\n       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n       '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n       'HalfBath', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'MiscVal', 'TotalSF', 'Total_sqr_footage',\n       'Total_porch_sf', 'haspool', 'hasgarage', 'hasbsmt'],\n      dtype='object')\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Categoric features encoding and splitting to train and test data\n</h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) which returns kind of One-Hot encoded categoric features</li>\n        <li> Splitted to X and X_test by y length</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kind of One-Hot encoding\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\n# Spliting the data back to train(X,y) and test(X_sub)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X):, :]\nprint('Features size for train(X,y) and test(X_test):')\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)","execution_count":70,"outputs":[{"output_type":"stream","text":"Features size for train(X,y) and test(X_test):\nX (1460, 326) y (1460,) X_test (1459, 326)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    ML part (models initialization)\n</h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) as encoder which returns kind of One-Hot encoded categoric features</li>\n        <li> Splitted to X and X_test by y length</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n# check maybe 10 kfolds would be better\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=14, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n# Gradient Boosting for regression\ngboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)\n\n# LightGBM regressor.\nlgbm = lgb.LGBMRegressor(objective='regression', num_leaves=5,\n                         learning_rate=0.05, n_estimators=720,\n                         max_bin=55, bagging_fraction=0.8,\n                         bagging_freq=5, feature_fraction=0.2319,\n                         feature_fraction_seed=9, bagging_seed=9,\n                         min_data_in_leaf=6, min_sum_hessian_in_leaf=11)\n\n# optimal parameters, received from CV\nc_grid = {\"n_estimators\": [1000],\n          \"early_stopping_rounds\": [1],\n          \"learning_rate\": [0.1]}\nxgb_regressor = XGBRegressor(objective='reg:squarederror')\ncross_validation = KFold(n_splits=5, shuffle=True, random_state=2)\nxgb_r = GridSearchCV(estimator=xgb_regressor,\n                     param_grid=c_grid,\n                     cv=cross_validation)\n\n# TODO add more regressors? How is this works?\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, lgbm, gboost),\n                                meta_regressor=elasticnet,\n                                use_features_in_secondary=True)\n\nsvr = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))","execution_count":71,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    ML part (models fitting)\n</h1>\n<span> Briefly:\n    <ul>\n        <li> One-by-one all models fitting</li>\n        <li> Printing models scores (might be commented for quicker work) </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n\\nFitting our models ensemble: ')\nprint('Elasticnet is fitting now...')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso is fitting now...')\nlasso_model = lasso.fit(X, y)\nprint('Ridge is fitting now...')\nridge_model = ridge.fit(X, y)\nprint('XGB is fitting now...')\nxgb_model = xgb_r.fit(X, y)\nprint('Gradient Boosting regressor is fitting now...')\ngboost_model = gboost.fit(X, y)\nprint('LGBMRegressor is fitting now...')\nlgbm_model = lgbm.fit(X, y)\nprint('stack_gen is fitting now...')\nstack_gen_model = stack_gen.fit(X, y)\nprint('SVR is fitting now...')\nsvr_model = svr.fit(X, y)\n\n\n# # model scoring and validation function\n# def cv_rmse(the_model, x):\n#     return np.sqrt(-cross_val_score(the_model, x, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n\n\n# print('\\n\\nModels evaluating: ')\n# score = cv_rmse(ridge_model, X)\n# print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(lasso_model, X)\n# print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(elastic_model, X)\n# print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(xgb_model, X)\n# print(\"xgb_r score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(gboost_model, X)\n# print(\"Gradient boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(lgbm_model, X)\n# print(\"LGB score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(stack_gen_model, X)\n# print(\"Stack gen score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n# score = cv_rmse(svr_model, X)\n# print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":72,"outputs":[{"output_type":"stream","text":"\n\nFitting our models ensemble: \nElasticnet is fitting now...\nLasso is fitting now...\nRidge is fitting now...\nXGB is fitting now...\nGradient Boosting regressor is fitting now...\nLGBMRegressor is fitting now...\nstack_gen is fitting now...\nSVR is fitting now...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    ML part (models ensembling)\n</h1>\n<span> Briefly:\n    <ul>\n        <li> The weighted sum of models on the basis of which the solution is assembled</li>\n        <li> There is score in comment to each row which explains coefficient to model</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend_models(x):\n    return (0.30 * stack_gen_model.predict(x) +    # 0.1236 (0.0232)\n            0.20 * gboost_model.predict(x) +      # 0.1249 (0.0208)\n            0.15 * elastic_model.predict(x) +    # 0.1266 (0.0221)\n            0.15 * lasso_model.predict(x) +     # 0.1267 (0.0221)\n            0.10 * lgbm_model.predict(x) +     # 0.1272 (0.0198)\n            0.05 * svr_model.predict(x) +     # 0.1273 (0.0247)\n            #  0.03 * xgb_r.predict(x) +     # 0.1283 (0.0168)\n            0.05 * ridge_model.predict(x))  # 0.1301 (0.0216)\n            \n\ndef rmsle(y_actual, y_pred):\n    return np.sqrt(mean_squared_error(y_actual, y_pred))\n\n\nprint('\\nRMSLE score on train data:')\nprint(rmsle(y, blend_models(X)))","execution_count":73,"outputs":[{"output_type":"stream","text":"\nRMSLE score on train data:\n0.0874830247333214\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>Model improvements history to see how various changes affect the performance of the model</h1>\n<span>\n<ul>\n    <li>**1. Initial start only with XGBoost**:\n        <ul>\n            <li>RMSLE: 0.13901</li>\n        </ul>\n    </li>\n    <li>**2. Added 'SalePrice' logarithming**:\n        <ul>\n            <li>RMSLE: 0.13984 ↗</li>\n            <li>Review: For now it made performance worse. Reasons are unknown.</li>\n        </ul>\n    </li>\n    <li>**3. Added huge amount of feature engineering**:\n        <ul>\n            <li>RMSLE: 0.13490 ↘</li>\n            <li>Review: Obviously this makes sense</li>\n        </ul>\n    </li>\n    <li>**4. Used Label encoder instead of get_dummies()**:\n        <ul>\n            <li>RMSLE: 0.13630 ↗</li>\n            <li>Review: get_dummies performs better by it's kind of OH-encoding</li>\n        </ul>\n    </li>\n    <li>**5. Aded skew-features fix**:\n        <ul>\n            <li>RMSLE: 0.13494 ↗</li>\n            <li>Review: It is just a bit better than 0.13490 (launch after feature engineering). I will comment it and uncomment a bit after</li>\n        </ul>\n    </li>\n    <li>**6. Created models ensemble: ridge, lasso, elasticnet, gbr, xgboost **:\n        <ul>\n            <li>RMSLE: 0.11759 ↘</li>\n            <li>Review: Sure. Models ensemble would make magic.</li>\n        </ul>\n    </li>\n    <li>**7. Added KernelRidge, Gradient Boosting and LGBMRegressor to ensemble and sum made weighted**:\n        <ul>\n            <li>RMSLE: 0.11638 ↘</li>\n            <li>Review: Very nice! Keep going!</li>\n        </ul>\n    </li>\n     <li>**8. Added SVR and Stack Gen**:\n        <ul>\n            <li>RMSLE: 0.11534 ↘</li>\n            <li>Review: Nice as well, but here comes question with coefficients.</li>\n        </ul>\n    </li>\n</ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:, 1] = np.expm1(blend_models(X_test))\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file is formed\")","execution_count":74,"outputs":[{"output_type":"stream","text":"Submission file is formed\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}